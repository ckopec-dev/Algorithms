# C4.5 Algorithm Implementation in Python

The C4.5 algorithm is a classic decision tree learning algorithm that uses information gain ratio to select attributes for splitting nodes.

```python
import math
import pandas as pd
from collections import Counter

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature      # Feature index to split on
        self.threshold = threshold  # Threshold value for splitting
        self.left = left           # Left child node
        self.right = right         # Right child node
        self.value = value         # Class label (for leaf nodes)

class C45DecisionTree:
    def __init__(self, max_depth=10):
        self.max_depth = max_depth
        self.root = None
    
    def entropy(self, y):
        """Calculate entropy of a dataset"""
        if len(y) == 0:
            return 0
        
        counts = Counter(y)
        entropy = 0
        total = len(y)
        
        for count in counts.values():
            probability = count / total
            if probability > 0:
                entropy -= probability * math.log2(probability)
        
        return entropy
    
    def information_gain(self, X_column, y, threshold):
        """Calculate information gain for a split"""
        # Split data based on threshold
        left_mask = X_column <= threshold
        right_mask = ~left_mask
        
        left_y = y[left_mask]
        right_y = y[right_mask]
        
        if len(left_y) == 0 or len(right_y) == 0:
            return 0
        
        # Calculate weighted entropy after split
        n = len(y)
        n_left, n_right = len(left_y), len(right_y)
        
        left_entropy = self.entropy(left_y)
        right_entropy = self.entropy(right_y)
        
        weighted_entropy = (n_left/n) * left_entropy + (n_right/n) * right_entropy
        
        # Information gain is entropy before minus weighted entropy after
        original_entropy = self.entropy(y)
        return original_entropy - weighted_entropy
    
    def information_gain_ratio(self, X_column, y, threshold):
        """Calculate information gain ratio (C4.5 approach)"""
        # Calculate information gain
        gain = self.information_gain(X_column, y, threshold)
        
        # Calculate split information
        left_mask = X_column <= threshold
        right_mask = ~left_mask
        
        left_y = y[left_mask]
        right_y = y[right_mask]
        
        if len(left_y) == 0 or len(right_y) == 0:
            return 0
        
        n = len(y)
        n_left, n_right = len(left_y), len(right_y)
        
        split_info = -(n_left/n) * math.log2(n_left/n) - (n_right/n) * math.log2(n_right/n)
        
        # Avoid division by zero
        if split_info == 0:
            return 0
            
        return gain / split_info
    
    def best_split(self, X, y):
        """Find the best feature and threshold to split on"""
        best_gain_ratio = -1
        best_feature = None
        best_threshold = None
        
        n_features = X.shape[1]
        
        for feature_idx in range(n_features):
            feature_values = X[:, feature_idx]
            thresholds = set(feature_values)
            
            for threshold in thresholds:
                gain_ratio = self.information_gain_ratio(feature_values, y, threshold)
                
                if gain_ratio > best_gain_ratio:
                    best_gain_ratio = gain_ratio
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_gain_ratio
    
    def build_tree(self, X, y, depth=0):
        """Recursively build the decision tree"""
        n_samples, n_features = X.shape
        n_classes = len(set(y))
        
        # Stopping criteria
        if depth >= self.max_depth or n_classes == 1 or n_samples == 0:
            # Return leaf node with most common class
            most_common = Counter(y).most_common(1)[0][0]
            return Node(value=most_common)
        
        # Find best split
        feature, threshold, gain_ratio = self.best_split(X, y)
        
        if feature is None or gain_ratio <= 0:
            # No good split found
            most_common = Counter(y).most_common(1)[0][0]
            return Node(value=most_common)
        
        # Split the data
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        left_X, left_y = X[left_mask], y[left_mask]
        right_X, right_y = X[right_mask], y[right_mask]
        
        # Create child nodes
        left_child = self.build_tree(left_X, left_y, depth + 1)
        right_child = self.build_tree(right_X, right_y, depth + 1)
        
        return Node(feature=feature, threshold=threshold, left=left_child, right=right_child)
    
    def fit(self, X, y):
        """Train the decision tree"""
        if isinstance(X, pd.DataFrame):
            X = X.values
        if isinstance(y, pd.Series):
            y = y.values
            
        self.root = self.build_tree(X, y)
    
    def predict_sample(self, x, node):
        """Predict a single sample"""
        if node.value is not None:
            return node.value
        
        if x[node.feature] <= node.threshold:
            return self.predict_sample(x, node.left)
        else:
            return self.predict_sample(x, node.right)
    
    def predict(self, X):
        """Predict multiple samples"""
        if isinstance(X, pd.DataFrame):
            X = X.values
            
        predictions = []
        for sample in X:
            prediction = self.predict_sample(sample, self.root)
            predictions.append(prediction)
        
        return predictions

# Example usage with a simple dataset
def example_usage():
    # Create sample data
    # Features: [age, income, student, credit_rating]
    # Target: buy_computer (yes/no)
    data = {
        'age': [25, 35, 45, 23, 35, 35, 26, 48, 50, 31, 32, 29, 40, 35, 27],
        'income': [45, 80, 110, 20, 85, 80, 25, 100, 120, 85, 90, 65, 115, 95, 30],
        'student': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0],
        'credit_rating': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0],
        'buy_computer': [0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0]
    }
    
    df = pd.DataFrame(data)
    X = df[['age', 'income', 'student', 'credit_rating']]
    y = df['buy_computer']
    
    # Create and train the model
    tree = C45DecisionTree(max_depth=5)
    tree.fit(X, y)
    
    # Make predictions
    predictions = tree.predict(X)
    print("Predictions:", predictions)
    print("Actual:", y.values.tolist())
    
    # Show some sample predictions
    sample_data = [[25, 50, 1, 1], [35, 90, 0, 0]]
    sample_predictions = tree.predict(sample_data)
    print("Sample predictions for new data:", sample_predictions)

# Run the example
if __name__ == "__main__":
    example_usage()
```

This implementation includes:

1. **Node class**: Represents a node in the decision tree with feature, threshold, and child nodes
2. **C45DecisionTree class**: Main class implementing the C4.5 algorithm
3. **Key methods**:
   - `entropy()`: Calculates entropy of dataset
   - `information_gain()`: Computes information gain for splits
   - `information_gain_ratio()`: Implements C4.5's information gain ratio
   - `best_split()`: Finds optimal feature and threshold to split on
   - `build_tree()`: Recursively builds the decision tree
   - `predict()`: Makes predictions on new data

The algorithm:
- Uses information gain ratio instead of just information gain (C4.5 improvement)
- Handles continuous features by finding optimal thresholds
- Includes stopping criteria (max depth, pure nodes)
- Works with both numerical and categorical data

